\section{Results}

\begin{table}[htbp]
  \centering
  \begin{tabular}{lc}
    \toprule
    \textbf{Model} &WER (no LM)\\
    \midrule
    Whisper Large V2 \cite{whisperv2} &2.7* \\
    \midrule
    Wav2Vec 2.0 \cite{wav2vec2} HuggingFace &3.384 \\
    ClusterModel(K=4) & 3.585 \\
    ClusterModel(K=8) & 3.608 \\
    AttentionLoRA(K=4) & - \\
    \bottomrule
  \end{tabular}
  \caption{Word Error Rate (WER) of various models on the test.clean partition of LibriSpeech. Lower values indicate better performance. A * indicates that the result was reported by a different paper.}
  \label{tab:video_descriptor_comparison}
\end{table}

Results are shown in Table \ref{tab:video_descriptor_comparison}.

\subsection{Per-Speaker Performance}

Since the proposed AttentionLoRA and ClusterModel models are designed to be speaker-dependent,
we provide a brief analysis on per-speaker performance to better understand these systems.

\begin{figure}[h]
      \centering
      \includegraphics[width=0.45\textwidth]{figures/wer_delta.png}
      \caption{Difference in WER between ClusterModels and Wav2Vec 2.0 by speaker on LibriSpeech test-clean.
        Blue dots are the ClusterModel with $K=4$, and orange dots are the ClusterModel with $K=8$.
      The horizontal blue line represents 0, no difference in WER.}
      \label{fig:wer_delta}
\end{figure}


Figure \ref{TODO} shows the change in WER by speaker, between the original base wav2vec 2.0 model and the ClusterModel(K=4) model.
