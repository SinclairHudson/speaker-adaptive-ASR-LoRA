\section{Introduction}

TODO

Recently, automatic speech recognition (ASR) systems have achieved impressive results benchmark datasets such as LibriSpeech \cite{librispeech} and TIMIT \cite{timit}.
These systems are typically trained first in an unsupervised manner on a large corpus of audio data, and then fine-tuned on a smaller supervised dataset \cite{wav2vec2, TODO}.
Both the unsupervised and supervised training data contain a wide variety of speakers.
\subsection{Motivation}

As shown in figure \ref{fig:by_speaker}, word error rate varies by speaker, with some speakers having double the error rate when compared to others.
This 

\begin{figure}[h]
      \centering
      \includegraphics[width=0.4\textwidth]{figures/msr-vtt-length-histogram.png}
      \caption{
          Word Error Rate (WER) of by speaker on LibriSpeech test-clean. 
          Results are based on the Wav2Vec 2.0 model \cite{wav2vec2}.
          Points are individual speakers, and the red line is the WER computed over the whole test-clean split.
          Variance in WER increases as the number of utterances per speaker decreases.
  }
      \label{fig:by_speaker}
\end{figure}

In this paper, we explore two research questions:
\textbf{RQ1}: Can we improve the performance of an ASR system by incorporating speaker identity information?
\textbf{RQ2}: Can an ASR system learn an explicit prior to improve performance on under-represented speakers?

