\section{Method}

Below, we outline our two proposed systems.
The first is an attempt to address \textbf{RQ1}, and the second is an attempt to address \textbf{RQ2}.

\subsection{ASR by Speaker Clusters}

To improve the performance of an ASR system, we propose clustering utterances based on speaker identity features, 
and then training a separate adapter models for each cluster.
The idea is that each adapter will be able to specialize on the patterns of the speakers in that cluster, yielding better performance overall.
At test time, the utterance is assigned a cluster and then the corresponding adapter model is used to transcribe the utterance.
We call this approach the \textbf{ClusterModel}, and experiment with $K=4$, $K=8$, and $K=12$ clusters.

First, the training dataset is divided according to speaker features. To accomplish this, the frozen, pre-trained WeSpeaker \cite{wespeaker} speaker embedding model is used.
All utterances in the \verb|train-clean-100| split of LibriSpeech \cite{librispeech} were embedded using the WeSpeaker model.
The embeddings were then clustered using K-means with $K$ clusters. 
The centroid of each cluster was saved for inference.
Each cluster was used to train a separate LoRA adapter on top of a frozen wav2vec 2.0 \cite{wav2vec2} model.
Per cluster, 10\% was held out for validation since creating smaller, less noisy clusters makes the system particularly prone to overfitting.
There is no overlap between the speakers in the training and test sets of librispeech, so generalization to unseen speakers critical.
For the $K=4$ model, 2 epochs per cluster was sufficient to reach the minimal validation loss.
The $K=8$ model and $K=12$ model only required a single epoch per cluster.

At test time, the speaker embedding of the input utterance is computed using the WeSpeaker model.
Then, the utterance is assigned to a cluster by finding the closest cluster centroid, based on euclidean distance.
The corresponding LoRA adapter on top of the wav2vec 2.0 model is used to transcribe the utterance.
See Figure \ref{fig:clustermodel} for an overview of the ClusterModel system, both during training and inference.

\begin{figure}[h]
      \centering
      \includegraphics[width=0.45\textwidth]{figures/clustermodel.png}
      \caption{Block diagram of the ClusterModel($K=4$) training procedure (left), and inference procedure (right).}
      \label{fig:clustermodel}
\end{figure}

\subsection{AttentionLoRA: Learning Clusters}

The ClusterModel system is limited in that it can only cluster speakers based on the WeSpeaker embedding space.
While this provides the system with a useful speaker identity prior, it is possible that a more helpful partitioning or classification of utterances exists.
To create a model with the capacity to learn a more useful partitioning of the data, we propose the \textbf{AttentionLoRA} model.
The AttentionLoRA model contains $K$ specialized LoRA adapters, which are fused by an attention mechanism based on a "selector network" that takes the utterance as input.
This fused adapter is then attached to a frozen base model for improved inference.
The selector network is initialized as the WeSpeaker speaker embedding model, but is fine-tuned during training.
Because the selector network is initialized using a speaker embedding model, AttentionLoRA will likely "cluster" data based on speaker features initially.
However, since the selector network is trainable, it is possible that the model will learn to cluster data based on other latent features of the data.
See Figure \ref{fig:attentionlora} for an overview of the AttentionLoRA system.


Apart from the frozen base wav2vec 2.0 model, the AttentionLoRA model is trained end-to-end.
The gradients that propagate to the merged LoRA can be propagated to the individual LoRAs, the key vectors, and the speaker embedding model.
Additionally, a pairwise contrastive loss is added for the learned key vectors, to prevent the collapse of the key vectors to a single vector.
This loss penalises high cosine similarity between each pair of key vectors.
%with the key vectors stored in a matrix $C \in \mathbb{R}^{K \times D}$, the loss is defined as:

%$$ L(K) = \frac{1}{2} (1- KK^T)^2$$

\begin{figure}
      \centering
      \includegraphics[width=0.45\textwidth]{figures/attentionlora.png}
      \caption{AttentionLoRA model architecture. A speaker embedding model is used to generate a query vector.
      That query vector is used to attend over the different LoRAs of the model, producing a merged LoRA, which is attached to the frozen base model before end-to-end ASR inference.}
      \label{fig:attentionlora}
\end{figure}

\subsection{Training Details}
For both models, the all LoRA adapters had a rank of 8, and the base wav2vec 2.0 model was frozen. 
wav2vec 2.0 model from the HuggingFace Model Hub \cite{huggingface} served as the base model for all experiments, and was pre-trained on the 960h LibriSpeech dataset.
The query projection, key projection, value projection, and FFN weight matrices were the target modules for the LoRA adapters.
All experiments were conducted using a single NVIDIA RTX 3090 GPU, and HuggingFace implementations of wav2vec 2.0 and LoRA.
For more information, see the code repository at \url{https://github.com/SinclairHudson/speaker-adaptive-ASR-LoRA}.
