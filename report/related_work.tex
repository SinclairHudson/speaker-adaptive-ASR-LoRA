\section{Related Work}

Below, we outline three important areas of related work, which are necessary to understand the context of our work.

\subsection{Low-Rank Adaptation (LoRA)}

This work makes use of the Low-Rank Adaptation (LoRA) technique to improve the performance of large pre-trained ASR models.
LoRA is a parameter-efficient method for adapting a pre-trained model to a new task or domain \cite{lora}.
It functions by injecting trainable rank decomposition matrices into the model's weights, which are learned while the base model's weights are frozen.
This greatly reduces the number of parameters that need to be learned, while still benefiting from base model's pre-training.
While originally applied to large language models \cite{lora}, the LoRA technique is directly applicable to acoustic models in ASR that use transformer layers, such as wav2vec 2.0 and HuBERT \cite{wav2vec2, hubert}.
that use Transformer \cite{transformer} layers.
Once trained, LoRA weights are amenable to combination with the base model; the weights can simply be added to the base model's weights.
Complementary to this work, Yu et. al explore applying LoRA adapters to language models for re-scoring acoustic model outputs \cite{loraonlm}.

\subsection{Speaker-Adaptive ASR}

Speaker-Adaptive ASR systems aim to improve the performance of an ASR system by incorporating speaker identity information \cite{speakeradaptation}.
Similar to this work, \cite{childspeech} has successfully applied LoRA methods to improve the ASR performance of the Whisper \cite{whisper} acoustic model on a low-resource language.
\cite{speakeradaptation}


\subsection{Speaker Disentanglement}

Speaker disentanglement attempts to build representations that are invariant to speaker identity, while preserving information critical to content-related downstream tasks.
Emprically, representations learned via self-supervised learning (SSL) perform extremely well on content-related tasks as well as speaker-related tasks such as speaker identification.
This can be seen on the SUPERB benchmark \cite{}, where representations produced by models such as wav2vec 2.0 and HuBERT \cite{wav2vec2, hubert} perform very well on both speaker identification and content-related tasks such as ASR.
These results imply that these learned representations contain both content and speaker information, the latter of which is not useful for tasks such as ASR.
Systems such as ContentVec \cite{contentvec} and SPIN \cite{spin} attempt to remove speaker information from learned representations, while preserving content information.
By removing speaker-related information, these systems successfully improve performance of these representations on downstream tasks such as ASR and TODO \cite{contentvec, spin}.
TODO .... content loss, what's the problem with speaker disentanglement?
In this paper we take the opposite approach, explicitly modeling speaker identity in an attempt to improve performance on content-related tasks.
Instead of removing speaker information, this work explicitly models different speakers and adapts the model to the speaker.
